Milestone 2
Team Name: Team Baddie
Team Members: Brian Leung, Cayden Calo, Jeremiah Carlo Miguel, Travis Takushi
1. Data Preparation
a. Data reduction, data cleansing and data transformation steps:
Data Reduction: We made a smaller test file from the big Amazon data file. We took
product information and copied it to a new file called amazon-meta-test.txt until it was
about 10 MB. We did this in create_test_data.py.
Pseudo-code for data reduction:
function make_smaller_dataset(big_file, size_wanted):
open big_file to read
open new_file to write
current_size = 0
while current_size < size_wanted and more data exists:
product_info = read_one_product()
if product_info is good:
write product_info to new_file
current_size = current_size + size of product_info
close files
Data Cleansing: We cleaned up the product data to make it better. We removed extra
spaces, fixed bad data, and got rid of products that were missing important information like
ID numbers.
Pseudo-code for data cleansing:
function clean_up_products(all_products):
good_products = []
for each product:
if product missing ID or ASIN number:
skip this product
if product has title:
remove extra spaces
if title too long: cut it shorter
if product has rating:
make sure rating is between 0 and 5
add product to good_products
return good_products
Data Transformation: We changed the raw product data into organized lists that work
better with our database. We separated everything into products, reviews, categories, and
connections between products.
Pseudo-code for data transformation:
function organize_data(products):
clean_products = clean_up_products(products)
all_reviews = []
all_categories = []
product_connections = []
for each product in clean_products:
for each review of this product:
add product ID to review
add review to all_reviews
for each category this product belongs to:
add category to all_categories
for each similar product:
add connection between products
return organized lists of products, reviews, categories, connections
b. Parser algorithm description:
Our parser reads the Amazon data file line by line. It looks for blank lines to know when one
product ends and another begins. For each product, it finds information like ID, title, and
reviews.
Pseudo-code for parser:
function read_amazon_file(file_name):
all_products = []
current_product = empty
current_reviews = []
for each line in file:
remove extra spaces from line
if line is empty:
if we have a product:
add reviews to product
add product to all_products
start new product
continue to next line
if line starts with "Id:":
get the ID number
if line starts with "ASIN:":
get the ASIN code
if line has "title:":
get the product title
if line has "similar:":
find similar products
if line has "categories:":
find what categories this product belongs to
if line has "reviews:":
find all the customer reviews
if we still have a product at the end:
add it to all_products
return all_products
2. Database System
a. NoSQL database choice and scalability:
We chose Neo4j as our database. Neo4j is a graph database, which means it excels at
storing data that has many interconnected relationships. This works well for Amazon data
because products are connected to customers through reviews, products are linked to
other similar products, and products belong to different categories.
Scalability challenges: While Neo4j handles our 10 MB test dataset effectively, scaling to
the full Amazon dataset would present challenges. The database performance would
degrade significantly when querying complex product connections.
b. Non-relational schema design:
Our database has four types of nodes:
Types of nodes:
- Product: stores id, asin, title, group, salesrank, avg_rating
- Customer: stores customer id
- Category: stores category name
- Review: stores review id, rating, date, helpful_votes, total_votes, summary, text
Types of connections:
- Customer REVIEWED Product: shows which customers reviewed which products
- Product BELONGS_TO Category: shows what category each product is in
- Product SIMILAR_TO Product: shows which products are similar to each other
- Product CO_PURCHASED_WITH Product: shows which products people buy together
This design works well because it matches how Amazon data is naturally connected. We
can easily ask questions like "what other products are similar to this one?" or "what did
customers who bought this also buy?"
3. Data Ingestion and Query
a. Data ingestion and validation process:
We put the data into Neo4j using our simple_import.py script. Here's what it does:
1. Read the test file (amazon-meta-test.txt) and turn it into Python objects
2. Clean up the data and organize it into separate lists
3. Connect to the Neo4j database using our login information
4. Create indexes to make searches faster
5. Add all the nodes to the database in groups of 1000
6. Create all the connections between nodes
7. Run some test queries to make sure everything worked right
Test queries we used to check our work:
Count how many products: MATCH (p:Product) RETURN count(p)
Count how many customers: MATCH (c:Customer) RETURN count(c)
Count how many reviews: MATCH (r:Review) RETURN count(r)
Count how many categories: MATCH (cat:Category) RETURN count(cat)
Count similar products: MATCH (p1:Product)-[r:SIMILAR_TO]->(p2:Product) RETURN
count(r)
Show some co-purchased products: MATCH (p:Product)-
[r:CO_PURCHASED_WITH]->(q:Product) RETURN p.asin, q.asin LIMIT 10
b. Performance results and scaling considerations:
How fast our system worked on the 10.15 MB test file:
- Reading the file: processed 327 products in 0.21 seconds (about 1,537 products per
second)
- File processing speed: 47.74 MB per second
- Cleaning data: cleaned 327 products and 5,887 reviews in 0.01 seconds
- Finding connections: found 1,446 connections between similar products
- Putting data in database: processed 1000 records at a time for good speed
How these numbers would scale with bigger data:
For the full dataset (977 MB with 548,552 total items): Our parser is selective - it skips a lot
of incomplete or bad product records. From our 10.15 MB test file, we only kept 327 good
products. If we apply this same rate to the full dataset, we'd probably end up with around
35,000 usable products instead of the full 548,552.
At 47.74 MB/second file processing, the 977 MB file would take about 20 seconds to read.
Processing 35,000 products at 1,537 products/second would take about 23 seconds. But
we'd still have hundreds of thousands of reviews to insert into the database, which would
be the real bottleneck taking several hours with our current approach.
4. Source Code
See Attachment