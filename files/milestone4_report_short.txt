Project Statement for Milestone 4
Team Baddie
Brian Leung, Cayden Calo, Jeremiah Carlo Miguel, Travis Takushi

================================================================================
1. USER INTERFACE AND DATA VISUALIZATION
================================================================================

1.a Describe the user interface and data visualization components of the software.
- GUI: Python Tkinter with 2 tabs (Complex Queries, Pattern Mining).
- Visualizations: Embedded Matplotlib charts in Pattern Mining tab.
  - Dual charts: Total Support (top) and Confidence (bottom).
  - Displays top patterns with product names, support, confidence.
- Interactivity: Fully typeable filters for queries (group, min/max rating, min/max reviews, min/max sales rank, limit) with buttons (Execute Query, Clear All). Pattern Mining provides two modes (Load Pre‑Generated Results, Mine Patterns Now) with dual‑panel charts.

================================================================================
2. USER QUERIES AND RESULTS
================================================================================

2.a Provide user queries and their results to describe functionality.
- Query 1: High‑Rated DVDs
  - Conditions: group='DVD', min_rating=4.5, min_reviews=50, k=20
  - Time: ~1.23s | Result: 20 DVDs (limit), avg rating ~4.7

- Query 2: Popular Books
  - Conditions: group='Book', min_reviews=100, k=20
  - Time: ~1.47s | Result: 20 books (limit), top has >1,100 reviews

- Query 3: Top Music Albums
  - Conditions: group='Music', min_rating=4.0, max_salesrank=100000, k=20
  - Time: ~1.35s | Result: 20 albums (limit), avg rating ~4.6

- Query 4: Highest Rated (All)
  - Conditions: min_rating=4.8, min_reviews=20, k=30
  - Time: ~1.18s | Result: 28 products, Books dominate

- Query 5: Excellent Sales Performance
  - Conditions: max_salesrank=10000, min_rating=4.0, k=50
  - Time: ~1.29s | Result: 50 products (limit), avg rating ~4.4

================================================================================
3. SCALABILITY
================================================================================

3.a NoSQL cluster storage and benchmarks; compare with non‑cluster.
- Database: Neo4j 4.4 Enterprise, 3‑core causal cluster.
- Storage Model (per core, replicated):
  - Nodes: Product 548,552; Customer 1,555,170; Category 26,057.
  - Relationships: REVIEWED 7,593,244; BELONGS_TO 12,964,535; SIMILAR 1,231,439.
- Ingestion (Bulk LOAD CSV):
  - Single node (Community): 551.3s total (23.9M elements, 43,386 elems/s).
  - Enterprise cluster (3 cores, writes to LEADER): 1,523.5s per core; 71.7M total elements written (replication). Effective total throughput ≈ 47,094 elems/s (~108% of single when accounting for 3x copies).
- Query performance (Enterprise cluster):
  - Simple top‑rated query (~100 results): ≈21.4s, <0.5% variance across cores.
  - Heavy aggregation (~50 results): ≈392s avg, <5% variance.
- Comparison: Single node faster for ingestion (no replication). Enterprise offers redundancy, HA, and consistent read scalability with near‑identical performance across cores.

3.b Hadoop/Spark cluster configuration and benchmarks; comparison.
- Implementation: PySpark 3.5.3 in local[*] mode with Neo4j Connector for Spark (org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3).
- Architecture: Direct data loading from Neo4j cluster into Spark DataFrames via connector, eliminating Python driver overhead.
- Algorithms: 
  - FP‑Growth (Spark MLlib) for frequent itemset mining on full dataset.
  - Distributed aggregation and filtering via Spark SQL.
- Benchmarks (local mode with Neo4j Connector):
  - Data loading: 7.6M reviews loaded from Neo4j in ~45s (169k reviews/sec).
  - Transaction creation: 1.5M customer transactions grouped in ~30s.
  - FP‑Growth mining: Full dataset (min_support=0.001) completes in ~2-3 minutes.
  - Pattern validation: Cross-validation of top 50 patterns in test set in ~20s.
- Comparison: Neo4j Connector enables processing 3,000x more customers (1.5M vs 500) with 50-100x speedup due to:
  - Parallel data loading (vs sequential Python driver queries).
  - Distributed Spark operations (vs single-threaded Python loops).
  - Native FP-Growth implementation (vs manual pair counting).

3.c Hardware used for scalability tests.
- Device: DESKTOP-TP7B022 (ThinkPad)
- CPU: AMD Ryzen 7 5700U with Radeon Graphics (1.80 GHz)
- RAM: 16.0 GB (14.8 GB usable)
- System: 64-bit OS, x64-based processor
- OS: Windows 11 Pro
- Docker: Docker Desktop for Windows (WSL2) running Neo4j cluster containers

================================================================================
4. SOURCE CODE
================================================================================

4.a Application prototype source code (ZIP provided separately, dataset excluded).
- GUI: `gui_app.py` (Tkinter + Matplotlib).
- Data ingestion: `scripts/stream_load_full.py`, `scripts/bulk_load_replicated.py`.
- Query algorithms: `scripts/run_algorithms_full.py`, `scripts/spark_query_algorithm.py`.
- Pattern mining: 
  - `scripts/copurchase_pattern_mining.py` (Python + Neo4j driver, sampled).
  - `scripts/spark_pattern_mining.py` (PySpark FP-Growth, local mode).
  - `scripts/spark_neo4j_pattern_mining.py` (Neo4j Connector for Spark, full dataset).
- Precomputed results: `scripts/pregenerate_results.py` → `gui_results.json`.
- Note: ZIP archive of source code (excluding CSV/data) accompanies this submission.