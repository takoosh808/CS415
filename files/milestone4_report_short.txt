Project Statement for Milestone 4

Team Baddie
Brian Leung, Cayden Calo, Jeremiah Carlo Miguel, Travis Takushi

1. USER INTERFACE AND DATA VISUALIZATION
1.a User Interface Description

Our team developed a full desktop GUI application using Python Tkinter (gui_simple.py). The interface is structured into two tabs, each supporting interactive user workflows and visualizations.

Tab 1 — Complex Product Queries

Features:

Interactive query form supporting:

Product group (Book / DVD / Music / Video)

Min/max rating

Min/max review count

Min/max sales rank

Result limit (k)

Automatic validation and exclusion of low-quality or missing data.

Buttons:

Execute Query — dynamically constructs and runs Cypher against Neo4j

Clear All — reset all fields

Output displayed in a scrollable results panel (title, ASIN, rating, reviews, salesrank).

Real-time status updates in a persistent bottom status bar.

Tab 2 — Co-Purchase Pattern Mining

Features:

Left panel: Sortable top-50 co-purchase pattern table (support, confidence, product titles).

Middle visualization:
Embedded Matplotlib bar charts:

Support of top 20 patterns

Confidence of top 20 patterns

Right panel (pattern details):

Product metadata

Ratings and titles

Sample customers (10 per pattern)

Two pie-chart visualizations showing rating distributions for the two products in the selected pattern.

“Load Patterns” button loads pregenerated results from gui_patterns.json instantly.

Visualization Components

Implemented using Matplotlib embedded via FigureCanvasTkAgg.

Visualization types:

Bar charts (support, confidence)

Pie charts (rating distribution)

Consistent formatting and responsive layout.

2. USER QUERIES AND RESULTS

Below are representative examples demonstrating the functionality and performance of the system.

Query	Conditions	Time	Results
High-Rated DVDs	group='DVD', min_rating=4.5, min_reviews=50, k=20	~1.23s	20 DVDs, avg rating ≈ 4.7
Popular Books	group='Book', min_reviews=100, k=20	~1.47s	20 books, top >1,100 reviews
Top Music Albums	group='Music', min_rating=4.0, max_salesrank=100000, k=20	~1.35s	20 albums, avg rating ≈ 4.6
Highest Rated Overall	min_rating=4.8, min_reviews=20, k=30	~1.18s	28 total products (Books dominate)
Excellent Sales Performance	max_salesrank=10000, min_rating=4.0, k=50	~1.29s	50 products, avg rating ≈ 4.4
3. SCALABILITY
3.a Neo4j NoSQL Cluster Storage & Performance
Cluster Configuration

Neo4j Enterprise 4.4, 3-core causal cluster (1 Leader, 2 Followers)

Data is Raft-replicated across all cores.

Dataset Loaded into the Cluster

Nodes:

Products: 548,552

Customers: 1,555,170

Categories: 26,057

Relationships:

REVIEWED: 7,593,244

BELONGS_TO: 12,964,535

SIMILAR: 1,231,439

Cluster Ingestion Performance (LOAD CSV, Replicated Write)

Total ingestion time: ~2,110 seconds (~35.2 minutes)

Examples:

Products: 53.9s (10,169/sec)

BELONGS_TO: 843.0s (15,379/sec)

REVIEWED: 1,016.7s (7,468/sec)

Throughput:

~11,335 elements/sec (nodes + relationships)

Replication to all 3 cores increases reliability but slows ingestion.

Single-Node Neo4j Performance (neo4j-admin import)

(Required Section 5 information has been merged here)

Configuration: Neo4j Enterprise 4.4.46, Java 11, 12.7GB RAM, SSD

Bulk import time: 111 seconds

Throughput:

Nodes: 19,188/sec

Relationships: 196,300/sec

Total: 215,488 elements/sec

Peak RAM: 1.05 GB

Result:
Single-node bulk import is 16–32× faster than cluster LOAD CSV due to no Raft replication.

Query Performance (Cluster)

Simple query (~100 results): ~21.4s

Heavy aggregation (top patterns, tens of millions of edges): ~392s avg

<5% performance variation across cores due to distributed read load.

Cluster vs Single-Node Summary
Metric	Single-Node	3-Core Cluster
Ingestion	Fastest (111s)	Much slower (2,110s) due to replication
Query Throughput	Moderate	Better distributed read performance
Fault Tolerance	None	High Availability
Best For	Analytics, development	Production, scalable reads
3.b Spark / Hadoop Cluster Algorithms & Benchmarks
Spark Environment

PySpark 4.0.1 (local[*])

Neo4j Connector for Apache Spark 5.3.1

Java 21

Reads automatically distributed across all 3 Neo4j cores.

Data Loading from Neo4j Cluster

7.59M REVIEWED rows

Load time: 307.48s

Throughput: 24,695 reviews/sec

Transaction Construction

810,216 unique customer transactions

Reduced to 49,763 (6.2%) via sampling

Ensured uniqueness with collect_set

FP-Growth Pattern Mining

Parameters:

min_support = 0.002

min_confidence = 0.1

Results:

Mining time: 110.61s

67 frequent itemsets

134 association rules

Strong patterns validated in test set (400+ support)

Full Pipeline Time

≈ 418 seconds
(Load → Mine → Validate → Enrich)

Advantages of Using Spark + Neo4j Connector

Analyzes 100× more transactions than Python driver version

Avoids memory crashes (DISK_ONLY caching)

Distributed reads across entire Neo4j cluster

Automatic load balancing and HA support

3.c Hardware Used

Laptop: ThinkPad / DESKTOP-TP7B022

CPU: AMD Ryzen 7 5700U (8 cores, 1.80GHz)

RAM: 16 GB

Disk: SSD

OS: Windows 11 Pro

Cluster Deployment: Docker Desktop + WSL2 running 3 Neo4j Enterprise nodes

4. SOURCE CODE
Provided in ZIP