Milestone 3 Report
Team Baddie — Brian Leung, Cayden Calo, Jeremiah Carlo Miguel, Travis Takushi

1. Data Files
=============================================================================

a. Distributed Data Files

We use Neo4j as the NoSQL database. No additional distributed data files (Parquet, HDFS) are used beyond the Neo4j database storage.

Data Transformation Steps:
1. Parse amazon-meta.txt file (932 MB)
2. Extract product attributes: ASIN, title, group, salesrank, avg_rating, total_reviews
3. Extract customer-product review relationships
4. Extract similar product links
5. Load into Neo4j using batch inserts (1,000 records per batch)
6. Create indexes on Product.asin and Product.id

Final Dataset:
- Products: 36,202
- Customers: 75,347
- Reviews: 85,551
- Similar relationships: 29,298

b. Sample Data

Product: Pulp Fiction (Special Collector's Edition)
ASIN: 630395345X
Group: DVD
Rating: 4.5
Reviews: 324

Product: Reservoir Dogs (10th Anniversary)
ASIN: B000068TZZ
Group: DVD
Rating: 4.3
Reviews: 198

2. Algorithm Description
=============================================================================

ALGORITHM 1: Complex Query Algorithm

a. Formal Description

Input:
- conditions: dictionary with optional keys (min_rating, max_rating, min_reviews, group, max_salesrank)
- k: number of results to return

Output:
- List of top k products matching conditions, sorted by rating and review count

Computing Operations:
1. Build WHERE clause from conditions
2. Execute parameterized Cypher query on Neo4j
3. Filter by rating, reviews, category, salesrank
4. Sort by avg_rating DESC, total_reviews DESC
5. Return top k results

b. Pseudo-code

function execute_query(conditions, k):
    where_clauses = []
    params = {}
    
    if 'min_rating' in conditions:
        where_clauses.append("p.avg_rating >= $min_rating")
        params['min_rating'] = conditions['min_rating']
    
    if 'group' in conditions:
        where_clauses.append("p.group = $group")
        params['group'] = conditions['group']
    
    if 'max_salesrank' in conditions:
        where_clauses.append("p.salesrank <= $max_salesrank AND p.salesrank > 0")
        params['max_salesrank'] = conditions['max_salesrank']
    
    query = "MATCH (p:Product) WHERE " + AND_join(where_clauses)
    query += " RETURN p ORDER BY p.avg_rating DESC, p.total_reviews DESC LIMIT $k"
    params['k'] = k
    
    results = neo4j.execute(query, params)
    return results

c. Optimization Techniques

- Database indexes on Product.asin and Product.id for fast lookups
- Parameterized queries reduce query planning overhead
- LIMIT clause prevents scanning unnecessary records
- Filtering done in database before returning results

ALGORITHM 2: Co-Purchasing Pattern Mining

a. Formal Description

Input:
- min_support: minimum customer count for frequent patterns
- max_customers: limit on customers analyzed
- dataset: 'train' or 'test'

Output:
- Frequent product pairs (patterns)
- Support and confidence for each pattern
- Customers who purchased both products in pattern

Computing Operations:
1. Split reviews into train (70%) and test (30%) sets
2. Collect customer transactions (products reviewed per customer)
3. Count individual product frequencies
4. Generate product pairs from transactions
5. Count pair frequencies
6. Filter pairs by min_support
7. Calculate confidence for each pair
8. Validate patterns in test set
9. Find customers matching top patterns

b. Pseudo-code

function mine_frequent_patterns(min_support, max_customers, dataset):
    transactions = query_transactions(dataset, max_customers)
    
    item_counts = {}
    for transaction in transactions:
        for item in transaction:
            item_counts[item] += 1
    
    frequent_items = filter(item_counts, count >= min_support)
    
    pair_counts = {}
    for transaction in transactions:
        items = filter(transaction, item in frequent_items)
        for i in range(len(items)):
            for j in range(i+1, len(items)):
                pair = sort([items[i], items[j]])
                pair_counts[pair] += 1
    
    patterns = []
    for pair, count in pair_counts:
        if count >= min_support:
            confidence = count / item_counts[pair[0]]
            patterns.append({
                'products': pair,
                'support': count,
                'confidence': confidence
            })
    
    return sort(patterns, key='support', reverse=True)

function validate_in_test(train_patterns):
    test_transactions = query_transactions('test', max_customers)
    
    for pattern in train_patterns:
        test_count = count_in_transactions(pattern, test_transactions)
        pattern['test_support'] = test_count
    
    return patterns

function find_matching_customers(product_pair):
    query = "MATCH (c:Customer)-[r1:REVIEWED]->(p1:Product {asin: $asin1})
             MATCH (c)-[r2:REVIEWED]->(p2:Product {asin: $asin2})
             WHERE r1.dataset = 'train' AND r2.dataset = 'train'
             RETURN c.id, r1.rating, r2.rating"
    
    return neo4j.execute(query, asin1=pair[0], asin2=pair[1])

c. Optimization Techniques

- Apriori principle: only consider high-support items for pairs
- Top-k customer sampling reduces computational load
- In-memory dictionaries for fast counting
- Neo4j graph traversals for efficient relationship queries
- Train/test split validates pattern generalizability

3. Algorithm Results
=============================================================================

a. Algorithm Output

ALGORITHM 1 - Query Algorithm

Test Query 1: min_rating=4.5, min_reviews=100, k=10
Results: 10 products found
Top result: "The Godfather - The Coppola Restoration" (Rating: 4.8, Reviews: 287)

Test Query 2: group='Book', min_rating=4.0, max_salesrank=50000, k=10
Results: 10 products found
Top result: "Harry Potter and the Sorcerer's Stone" (Rating: 4.7, Reviews: 1,142)

Test Query 3: group='Music', min_rating=4.0, min_reviews=50, k=10
Results: 10 products found
Top result: "The Beatles - Abbey Road" (Rating: 4.6, Reviews: 156)

ALGORITHM 2 - Pattern Mining

Training Set: 59,885 reviews (70%)
Testing Set: 25,666 reviews (30%)

Top Pattern:
Products: Pulp Fiction + Reservoir Dogs
Training Support: 14 customers
Testing Support: 2 customers
Confidence: 20.6%
Total Customers: 17

Sample Matching Customers:
1. A1O3AKBX6THKB1: Pulp Fiction (5 stars), Reservoir Dogs (5 stars)
2. A3DWXGYH7F990O: Pulp Fiction (1 star), Reservoir Dogs (3 stars)
3. A1KVSO9MQBQA8Y: Pulp Fiction (5 stars), Reservoir Dogs (4 stars)

b. Performance Metrics

ALGORITHM 1:
- Query 1 execution time: 1.2 seconds
- Query 2 execution time: 1.5 seconds
- Query 3 execution time: 1.3 seconds
- Average: 1.4 seconds

ALGORITHM 2:
- Train/test split: 30 seconds (85,551 relationships)
- Pattern mining: 0.75 seconds (500 customers)
- Customer identification: 0.2 seconds

c. Result Presentation

Query Algorithm: Executed on-demand based on user input filters. Results returned immediately for display.

Pattern Mining: Executed offline as batch process. Results stored in database or cache for fast retrieval in recommendation system ("Customers who bought X also bought Y").

4. Algorithm Scalability
=============================================================================

We implemented two versions:
1. Python + Neo4j (scripts/run_algorithms_full.py, scripts/copurchase_pattern_mining.py)
2. Apache Spark (scripts/spark_query_algorithm.py, scripts/spark_pattern_mining.py)

ALGORITHM 1 - Query Algorithm Scalability

The algorithm scales through:
- Indexed lookups: O(log n) scaling with data size
- Neo4j clustering support for horizontal scaling
- Spark DataFrame operations for distributed processing:
  * WHERE → filter()
  * ORDER BY → orderBy()
  * LIMIT → limit()
- Minimal data transfer (only k results)

Spark Implementation:
- Loads product data from Neo4j into Spark DataFrames
- Distributes filter operations across partitions
- Performs parallel sorting
- Aggregates top-k from each partition

Performance: 1.4s on 36K products (Python), scales to 2-5s on 5M products (Spark cluster)

ALGORITHM 2 - Pattern Mining Scalability

The algorithm scales through:
- Apriori pruning: eliminates low-support candidates early
- Customer sampling: processes subset for faster results
- Spark MLlib FP-Growth for distributed mining
- Parallel transaction processing

Spark Implementation:
- Uses Spark MLlib FP-Growth algorithm
- Distributes customer transactions across partitions
- Parallel FP-tree construction
- Map-reduce aggregation of frequent itemsets
- Generates association rules with lift and confidence

Performance: 30s for split + 0.75s mining (36K products), scales to 15-20 minutes on 5M products (Spark cluster)

5. Source Code
=============================================================================

Source code structure:

Project/
├── src/data_processing/
│   └── parser.py
├── scripts/
│   ├── check_db_status.py
│   ├── stream_load_full.py
│   ├── run_algorithms_full.py
│   ├── copurchase_pattern_mining.py
│   ├── spark_query_algorithm.py
│   ├── spark_pattern_mining.py
│   └── run_spark_algorithms.py
├── files/
│   ├── milestone3_report.txt
│   └── [other reports]
├── amazon-meta-10mb.txt
├── amazon-meta.txt
└── requirements.txt

Technologies:
- Python 3.11+
- Neo4j 5.x
- Apache Spark 3.5+ with MLlib
- PySpark

Source code provided in ZIP file.